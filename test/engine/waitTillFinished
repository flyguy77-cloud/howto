// Polling, perfect voor sequentieel uitvoeren. Robuust in productie, voor nu de voorkeur
// of hybride zie helemaal onderaan future icm database blijft werkelijkheid.


@Service
@RequiredArgsConstructor
public class JobExecutionService {

    public UUID executeScriptJob(RunScriptNodeRequestDto dto) {

        UUID execId = UUID.randomUUID();

        executeScriptJobInternal(dto, execId);  <<-- verzoek komt van buiten, dus execId moet gegnereerd worden eerst

        return execId;
    }
}


@Service
@RequiredArgsConstructor
public class JobExecutionService {

    private final JobExecutionRepository jobRepo;
    private final KubernetesClient client;
    private final JobMonitor jobMonitor;

    @Value("${kubernetes.client.namespace}")
    private String namespace;

    public void executeScriptJobInternal(RunScriptNodeRequestDto dto, UUID execId) {  <<-- voor jobRunr exec dat al een execId heeft

        // 1. init DB record
        JobExecutionEntity job = new JobExecutionEntity();
        job.setId(execId);
        job.setStatus(JobExecutionStatus.PENDING);
        jobRepo.save(job);

        // 2. create k8s job
        Job k8sJob = buildKubernetesJob(dto, execId);

        client.batch().v1().jobs()
                .inNamespace(namespace)
                .resource(k8sJob)
                .create();

        // 3. start watchers
        jobMonitor.start(
                namespace,
                k8sJob.getMetadata().getName(),
                execId,
                k8sJob.getMetadata().getUid()
        );

        // 4. WAIT (blocking)
        waitUntilJobFinished(execId);         <<---- wacht tot de job klaar is, ga dan door met de volgende (sequentieel)
    }
}

private void waitUntilJobFinished(UUID execId) {

    while (true) {
        JobExecutionStatus status = jobRepo.findById(execId)
                .map(JobExecutionEntity::getStatus)
                .orElseThrow();

        if (status == JobExecutionStatus.SUCCEEDED) {
            return;
        }

        if (status == JobExecutionStatus.FAILED) {
            throw new IllegalStateException(
                "Job failed: " + execId
            );
        }

        try {
            Thread.sleep(1000); // 1s poll     <<--- dit is de database poller, dat per 1s checkt. 1s is prima, 5s is to lang wachten
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
            throw new IllegalStateException("Interrupted", e);
        }
    }
}


// Completable future, voor parallele verwerking is dit het startpunt maar kan ook als poller gebruikt worden

@Component
public class JobCompletionRegistry {

    private final ConcurrentMap<UUID, CompletableFuture<JobExecutionStatus>> futures =
            new ConcurrentHashMap<>();

    public CompletableFuture<JobExecutionStatus> register(UUID execId) {
        CompletableFuture<JobExecutionStatus> future = new CompletableFuture<>();
        futures.put(execId, future);
        return future;
    }

    public void complete(UUID execId, JobExecutionStatus status) {
        CompletableFuture<JobExecutionStatus> future = futures.remove(execId);
        if (future != null) {
            future.complete(status);
        }
    }

    public void fail(UUID execId, Throwable t) {
        CompletableFuture<JobExecutionStatus> future = futures.remove(execId);
        if (future != null) {
            future.completeExceptionally(t);
        }
    }
}

// JobExecutionService met future

@Service
@RequiredArgsConstructor
public class JobExecutionService {

    private final JobCompletionRegistry registry;

    public void executeScriptJobInternal(
            RunScriptNodeRequestDto dto,
            UUID execId
    ) {

        CompletableFuture<JobExecutionStatus> future =
                registry.register(execId);

        startKubernetesJob(dto, execId);

        // BLOCK until watcher completes it
        JobExecutionStatus status = future.join();

        if (status == JobExecutionStatus.FAILED) {
            throw new IllegalStateException("Job failed " + execId);
        }
    }
}

// Afronden van future

@Component
@RequiredArgsConstructor
public class JobStatusWatcher {

    private final KubernetesClient client;
    private final JobStatusService statusService;
    private final JobCompletionRegistry registry;

    public Watch start(
            String namespace,
            String jobName,
            UUID execId,
            String jobUid
    ) {

        return client.batch().v1().jobs()
            .inNamespace(namespace)
            .withName(jobName)
            .watch(new Watcher<>() {

                @Override
                public void eventReceived(Action action, Job job) {

                    if (!job.getMetadata().getUid().equals(jobUid)) return;

                    JobExecutionStatus mapped = mapStatus(job);
                    statusService.updateStatus(execId, mapped, null);

                    if (mapped == JobExecutionStatus.SUCCEEDED ||
                        mapped == JobExecutionStatus.FAILED) {

                        registry.complete(execId, mapped);
                        close(); // Fabric8 Watch close
                    }
                }

                @Override
                public void onClose(WatcherException cause) {
                    if (cause != null) {
                        registry.fail(execId, cause);
                    }
                }
            });
    }
}

// Let op! Voor Completable Futures is Life Cycle management nodig. Futures zijn memory based. 
// Wat als de app restart, dan is de state complete weg, jobs worden niet opgeruimt
// Extra code als:
//
// if (future != null && !future.isDone()) {
//    future.complete(status);
// }
//
// is nodig, niet moeilijk maar wel extra verantwoordelijkheid



// Hybride aanpak:
CompletableFuture<JobStatus> future = registry.register(execId);

JobStatus status = future
    .orTimeout(30, MINUTES)    <<--- beter zonder harde timeout, of laat k8s de timeout regelen!
    .exceptionally(ex -> jobRepo.getStatus(execId))     <<--- dan zou deze ook niet nodig meer zijn
    .join();



=========

De kern van het probleem (in √©√©n zin)

gebruikt √©√©n type execId voor twee verschillende verantwoordelijkheden:
	‚Ä¢	workflow-level executie
	‚Ä¢	node/job-level executie

Daardoor:
	‚Ä¢	watchers kijken in de verkeerde tabel
	‚Ä¢	logs/status worden aan het verkeerde record gekoppeld
	‚Ä¢	je verliest referenti√´le samenhang

‚∏ª

De juiste mentale scheiding (dit is de sleutel)

twee soorten executies. Ze mogen nooit dezelfde identifier delen.

1 WorkflowExecution (macro-niveau)

‚û°Ô∏è ‚ÄúDeze workflow-run als geheel‚Äù
	‚Ä¢	workflowExecId
	‚Ä¢	1 per workflow start
	‚Ä¢	Wordt gebruikt door:
	‚Ä¢	WorkflowExecutionService
	‚Ä¢	WorkflowRunner
	‚Ä¢	voortgang van de workflow (STARTED / RUNNING / FINISHED / FAILED)

2 JobExecution (micro-niveau)

‚û°Ô∏è ‚ÄúDeze concrete K8s job / node-uitvoering‚Äù
	‚Ä¢	jobExecId
	‚Ä¢	1 per node-executie
	‚Ä¢	Wordt gebruikt door:
	‚Ä¢	JobExecutionService
	‚Ä¢	Fabric8 watchers (status/log/pod)
	‚Ä¢	K8s Job metadata

üëâ Dit zijn twee totaal verschillende concepten.

‚∏ª

Wat gaat er nu mis

‚Äúde podWatcher wil zijn log niet wegschrijven omdat deze execId niet in de tabel gevonden kan worden‚Äù

Dat betekent vrijwel altijd:
	‚Ä¢	watcher gebruikt workflowExecId
	‚Ä¢	repository verwacht jobExecId
	‚Ä¢	of andersom

üëâ Klassieke split brain.

‚∏ª

De structurele oplossing (zonder refactor-hel)

‚ú≥Ô∏è Regel 1 ‚Äî Workflow genereert all√©√©n workflowExecId

@Service
@RequiredArgsConstructor
public class WorkflowExecutionService {

    public UUID startExecution(UUID workflowId) {

        UUID workflowExecId = UUID.randomUUID();

        // sla WorkflowExecutionEntity op
        // status = RUNNING

        BackgroundJob.enqueue(() ->
            workflowRunner.runWorkflow(workflowId, workflowExecId)
        );

        return workflowExecId;
    }
}

üëâ WorkflowExecutionService kent g√©√©n jobExecId.

‚∏ª

‚ú≥Ô∏è Regel 2 ‚Äî Node-executie genereert z√©lf een jobExecId

En koppelt die expliciet aan de workflow.

@Service
@RequiredArgsConstructor
public class JobExecutionService {

    public UUID executeScriptJobInternal(
            RunScriptNodeRequestDto dto,
            UUID workflowExecId,
            UUID nodeId
    ) {

        UUID jobExecId = UUID.randomUUID();

        JobExecutionEntity job = new JobExecutionEntity();
        job.setId(jobExecId);
        job.setWorkflowExecId(workflowExecId); // üî• BELANGRIJK
        job.setNodeId(nodeId);
        job.setStatus(PENDING);

        jobRepo.save(job);

        // start K8s job + watchers met jobExecId
        startK8sJob(dto, jobExecId);

        return jobExecId;
    }
}

üëâ Watchers gebruiken altijd jobExecId.
üëâ WorkflowRunner gebruikt altijd workflowExecId.

‚∏ª

Cruciaal: watchers mogen NOOIT workflowExecId gebruiken

‚ùå FOUT (wat nu gebeurt)

jobMonitor.start(namespace, jobName, workflowExecId, jobUid);

‚úÖ GOED

jobMonitor.start(namespace, jobName, jobExecId, jobUid);

Want:
	‚Ä¢	logs/status horen bij job_execution
	‚Ä¢	niet bij workflow_execution

‚∏ª

Hoe de WorkflowRunner dit netjes verbindt

public void runNode(UUID workflowId, UUID workflowExecId, UUID nodeId) {

    NodeEntity node = nodeRepo.findById(nodeId).orElseThrow();

    UUID jobExecId =
        jobExecutionService.executeScriptJobInternal(
            mapNodeToDto(node),
            workflowExecId,
            nodeId
        );

    // ‚è≥ wacht tot deze job klaar is (polling / future)
    waitUntilJobFinished(jobExecId);

    // daarna ‚Üí volgende node
}

üëâ Workflow wacht op job,
üëâ job weet bij welke workflow hij hoort,
üëâ watchers zijn volledig lokaal per job.

‚∏ª

Datamodel (dit maakt alles stabiel)

WorkflowExecutionEntity

id (workflowExecId)
workflowId
status
startedAt
finishedAt

JobExecutionEntity

id (jobExecId)
workflowExecId  // FK
nodeId
status
logs
startedAt
finishedAt

Zo kun je:
	‚Ä¢	alle jobs van een workflow ophalen
	‚Ä¢	logs per node tonen
	‚Ä¢	workflow-status afleiden van jobs

‚∏ª

Waarom dit jouw probleem oplost

‚úî Geen execId-verwarring meer
‚úî Watchers vinden altijd hun record
‚úî Logs worden altijd geschreven
‚úî Workflow & jobs zijn los gekoppeld
‚úî Klaar voor parallel later

‚∏ª

Belangrijk inzicht (onthouden!)

WorkflowExecId = orchestratie

JobExecId = uitvoering

